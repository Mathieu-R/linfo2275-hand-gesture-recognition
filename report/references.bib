 @article{Salvador_Chan, title={FastDTW: Toward Accurate Dynamic Time Warping in Linear Time and Space}, abstractNote={The dynamic time warping (DTW) algorithm is able to find the optimal alignment between two time series. It is often used to determine time series similarity, classification, and to find corresponding regions between two time series. DTW has a quadratic time and space complexity that limits its use to only small time series data sets. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarse resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW compared to two other existing approximate DTW algorithms: Sakoe-Chuba Bands and Data Abstraction. Our results show a large improvement in accuracy over the existing methods.}, author={Salvador, Stan and Chan, Philip}, language={en} }

 @inproceedings{Wobbrock_Wilson_Li_2007, address={New York, NY, USA}, series={UIST ’07}, title={Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes}, ISBN={978-1-59593-679-0}, url={https://doi.org/10.1145/1294211.1294238}, DOI={10.1145/1294211.1294238}, abstractNote={Although mobile, tablet, large display, and tabletop computers increasingly present opportunities for using pen, finger, and wand gestures in user interfaces, implementing gesture recognition largely has been the privilege of pattern matching experts, not user interface prototypers. Although some user interface libraries and toolkits offer gesture recognizers, such infrastructure is often unavailable in design-oriented environments like Flash, scripting environments like JavaScript, or brand new off-desktop prototyping environments. To enable novice programmers to incorporate gestures into their UI prototypes, we present a “$1 recognizer” that is easy, cheap, and usable almost anywhere in about 100 lines of code. In a study comparing our $1 recognizer, Dynamic Time Warping, and the Rubine classifier on user-supplied gestures, we found that $1 obtains over 97% accuracy with only 1 loaded template and 99% accuracy with 3+ loaded templates. These results were nearly identical to DTW and superior to Rubine. In addition, we found that medium-speed gestures, in which users balanced speed and accuracy, were recognized better than slow or fast gestures for all three recognizers. We also discuss the effect that the number of templates or training examples has on recognition, the score falloff along recognizers’ N-best lists, and results for individual gestures. We include detailed pseudocode of the $1 recognizer to aid development, inspection, extension, and testing.}, booktitle={Proceedings of the 20th annual ACM symposium on User interface software and technology}, publisher={Association for Computing Machinery}, author={Wobbrock, Jacob O. and Wilson, Andrew D. and Li, Yang}, year={2007}, month={Oct}, pages={159–168}, collection={UIST ’07} }

  @inproceedings{Vatavu_Anthony_Wobbrock_2012, address={Santa Monica California USA}, title={Gestures as point clouds: a $P recognizer for user interface prototypes}, ISBN={978-1-4503-1467-1}, url={https://dl.acm.org/doi/10.1145/2388676.2388732}, DOI={10.1145/2388676.2388732}, abstractNote={Rapid prototyping of gesture interaction for emerging touch platforms requires that developers have access to fast, simple, and accurate gesture recognition approaches. The $family of recognizers ($1, $N) addresses this need, but the current most advanced of these, $N-Protractor, has signiﬁcant memory and execution costs due to its combinatoric gesture representation approach. We present $P, a new member of the $-family, that remedies this limitation by considering gestures as clouds of points. $P performs similarly to $1 on unistrokes and is superior to $N on multistrokes. Speciﬁcally, $P delivers >99% accuracy in user-dependent testing with 5+ training samples per gesture type and stays above 99% for user-independent tests when using data from 10 participants. We provide a pseudocode listing of $P to assist developers in porting it to their speciﬁc platform and a “cheat sheet” to aid developers in selecting the best member of the $-family for their speciﬁc application needs.}, booktitle={Proceedings of the 14th ACM international conference on Multimodal interaction}, publisher={ACM}, author={Vatavu, Radu-Daniel and Anthony, Lisa and Wobbrock, Jacob O.}, year={2012}, month={Oct}, pages={273–280}, language={en} }
